{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3111,
     "status": "ok",
     "timestamp": 1566067894000,
     "user": {
      "displayName": "Pranjal upadhyay",
      "photoUrl": "",
      "userId": "06624524539236247766"
     },
     "user_tz": -330
    },
    "id": "rHXn2qLG5WOY",
    "outputId": "41e82117-b927-4518-fadd-28dfbe93365f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\geuser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\geuser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import pickle\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_call([\"python\", '-m', 'pip', 'install', 'pandas']) # install pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\geuser\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pip._internal import main as pipmain\n",
    "pipmain(['install', 'pandas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    pipmain(['install', 'pandas'])\n",
    "    import pandas as pd\n",
    "    \n",
    "try:\n",
    "    import numpy as np\n",
    "except:\n",
    "    pipmain(['install', 'numpy'])\n",
    "    import numpy as np\n",
    "    \n",
    "try:\n",
    "    import re\n",
    "except:\n",
    "    pipmain(['install', 're'])\n",
    "    import re\n",
    "    \n",
    "try:\n",
    "    import  nltk\n",
    "except:\n",
    "    pipmain(['install', 'nltk'])\n",
    "    import  nltk\n",
    "    nltk.download()\n",
    "    \n",
    "try:\n",
    "    import  textblob\n",
    "except:\n",
    "    pipmain(['install', 'textblob'])\n",
    "    import  textblob\n",
    "    \n",
    "try:\n",
    "    import pickle\n",
    "except:\n",
    "    pipmain(['install', 'pickle'])\n",
    "    import pickle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\geuser\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "pipmain(['install', 'nltk'])\n",
    "import  nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from keras.models import Sequential\n",
    "    from keras import optimizers\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import LSTM\n",
    "except:\n",
    "    pipmain(['install', 'keras'])\n",
    "    pipmain(['install', 'tensorflow'])\n",
    "    from keras.models import Sequential\n",
    "    from keras import optimizers\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import LSTM\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "except:\n",
    "    pipmain(['install', 'sklearn'])\n",
    "    from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask==0.12.2 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from flask==0.12.2) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from flask==0.12.2) (0.15.4)\n",
      "Requirement already satisfied: Jinja2>=2.4 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from flask==0.12.2) (2.10.1)\n",
      "Requirement already satisfied: click>=2.0 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from flask==0.12.2) (7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from Jinja2>=2.4->flask==0.12.2) (1.1.1)\n",
      "Requirement already satisfied: flask-ngrok in c:\\users\\geuser\\anaconda3\\lib\\site-packages (0.0.25)\n",
      "Requirement already satisfied: requests in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from flask-ngrok) (2.22.0)\n",
      "Requirement already satisfied: Flask>=0.8 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from flask-ngrok) (0.12.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from requests->flask-ngrok) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from requests->flask-ngrok) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from requests->flask-ngrok) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from requests->flask-ngrok) (1.24.2)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-ngrok) (0.15.4)\n",
      "Requirement already satisfied: Jinja2>=2.4 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-ngrok) (2.10.1)\n",
      "Requirement already satisfied: click>=2.0 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-ngrok) (7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\geuser\\anaconda3\\lib\\site-packages (from Jinja2>=2.4->Flask>=0.8->flask-ngrok) (1.1.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipmain(['install', 'flask == 0.12.2'])\n",
    "pipmain(['install', 'flask-ngrok'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00SQPLDM5wkq"
   },
   "source": [
    "### Importing all required datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19387,
     "status": "ok",
     "timestamp": 1566067953650,
     "user": {
      "displayName": "Pranjal upadhyay",
      "photoUrl": "",
      "userId": "06624524539236247766"
     },
     "user_tz": -330
    },
    "id": "uUuuZ1FI5vb2",
    "outputId": "8e971645-0804-4841-a6a1-71816b2c040a"
   },
   "outputs": [],
   "source": [
    "# text_formulated_data = pd.read_excel(\"Data/combined_text_formulated_data_buyer.xlsx\")\n",
    "\n",
    "# data_used_to_train_rate_quantity_model = pd.read_excel(\"Data/avg_rate_prediction_data.xlsx\")\n",
    "\n",
    "# #cost_model_training_data =pd.read_excel(\"Data/costing_text_regression_keras_data.xlsx\")\n",
    "\n",
    "# data_used_to_train_label_classification_model= (pd.read_excel(\"Data/label_classification_data_aval.xlsx\")).dropna()\n",
    "\n",
    "# #data_used_to_train_label_classification_model= (pd.read_excel(\"/content/drive/My Drive/RDPL/Costing Engine/Data/label_classification_data2.xlsx\")).dropna()  #if label_classification_model2 is used\n",
    "# data_used_to_train_label_classification_model = data_used_to_train_label_classification_model.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# data_used_to_train_rate_quantity_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_used_to_train_label_classification_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVzD0lqk6_jg"
   },
   "source": [
    "# Importing Neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLZwlWOZ70Rk"
   },
   "source": [
    "## Importing label classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "def invert_dict_class_weights(target_dict):\n",
    "  inverted_dict={}\n",
    "  for item in target_dict.items():\n",
    "    inverted_dict[item[1]]= str(item[0])\n",
    "  \n",
    "  return inverted_dict\n",
    "#target_dict= {'cmt': 2, 'description': 1, 'fabric': 4, 'packing': 3, 'trims': 0}\n",
    "#invert_dict_class_weights(target_dict)={0:'trims', 1:'description', 2:'cmt', 3:'packing', 4:'fabric'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8095,
     "status": "ok",
     "timestamp": 1566067975508,
     "user": {
      "displayName": "Pranjal upadhyay",
      "photoUrl": "",
      "userId": "06624524539236247766"
     },
     "user_tz": -330
    },
    "id": "PBpnG-TF6-vY",
    "outputId": "a19b0a10-f64f-4689-b03c-679a7c4a3031"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0924 09:40:20.404441  5444 deprecation_wrapper.py:119] From C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0924 09:40:20.818433  5444 deprecation_wrapper.py:119] From C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0924 09:40:21.255928  5444 deprecation_wrapper.py:119] From C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0924 09:40:21.256928  5444 deprecation_wrapper.py:119] From C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0924 09:40:21.257929  5444 deprecation_wrapper.py:119] From C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded newly updated model from disk\n"
     ]
    }
   ],
   "source": [
    "#label_classification_model_aval.json:  label dictionary\n",
    "target_dict= {'cmt': 2, 'description': 1, 'fabric': 4, 'packing': 3, 'trims': 0}\n",
    "#n-gram upper limit =2\n",
    "#dataset used: label_classification_data_aval.xlsx\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create classification model\n",
    "#json_file = open('/content/drive/My Drive/RDPL/Costing Engine/models/label_classification_model1.json', 'r')\n",
    "try:\n",
    "    json_file = open('models/label_classification_model_updated.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    label_classification_model = model_from_json(loaded_model_json)\n",
    "    label_classification_model.load_weights('models/label_classification_model_updated.h5')\n",
    "    print(\"Loaded newly updated model from disk\")\n",
    "    \n",
    "except:\n",
    "    json_file = open('models/label_classification_model_aval.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    label_classification_model = model_from_json(loaded_model_json)\n",
    "    label_classification_model.load_weights('models/label_classification_model_aval.h5')\n",
    "    print(\"Loaded old unupdated model\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "############################################################## LABEL CLASSIFICATION VECTORIZER ##############################################################################\n",
    "def get_vectorizer(text_corpus,ngrams_upper_limit):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test â€” samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    text_corpus=[str (item) for item in text_corpus]\n",
    "     \n",
    "        \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, ngrams_upper_limit))\n",
    "    #tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.9)\n",
    "    tfidf_vectorizer.fit_transform(list(text_corpus))\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "label_classifier_vectorizer=False\n",
    "############################################################################################################################\n",
    "try:\n",
    "    with open('vectorizers/label_classifier_vectorizer.pk', 'rb') as saved_vectorizer_binary_file:\n",
    "        label_classifier_vectorizer=pickle.load(saved_vectorizer_binary_file)\n",
    "except:\n",
    "    print(\"Coundn't load newly updated stored vectorizer\")\n",
    "    text_corpus=list(data_used_to_train_label_classification_model[\"product_info\"])\n",
    "    label_classifier_vectorizer = get_vectorizer(list(data_used_to_train_label_classification_model[\"product_info\"]), ngrams_upper_limit=2)\n",
    "\n",
    "\n",
    "#############################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZqL3znn8cG_"
   },
   "source": [
    "## Importing items quantity prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4523,
     "status": "ok",
     "timestamp": 1566067983537,
     "user": {
      "displayName": "Pranjal upadhyay",
      "photoUrl": "",
      "userId": "06624524539236247766"
     },
     "user_tz": -330
    },
    "id": "kwmugxST6kPU",
    "outputId": "7651daaa-7eef-4688-ed67-2b568d9c2cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create classification model\n",
    "json_file = open('models/item_avgs_model_updated.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "item_avgs_model = model_from_json(loaded_model_json)\n",
    "json_file.close()\n",
    "\n",
    "# load weights into the classification model\n",
    "item_avgs_model.load_weights('models/item_avgs_model_updated.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOlJC2UM95dJ"
   },
   "source": [
    "## Importing the  price rates  prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3163,
     "status": "ok",
     "timestamp": 1566067993405,
     "user": {
      "displayName": "Pranjal upadhyay",
      "photoUrl": "",
      "userId": "06624524539236247766"
     },
     "user_tz": -330
    },
    "id": "pMohi_Iv9y9Y",
    "outputId": "cf878464-f45a-4f8d-c388-3e7c209441ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "json_file = open('models/item_rates_model_updated.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "item_rates_model = model_from_json(loaded_model_json)\n",
    "json_file.close()\n",
    "\n",
    "item_rates_model.load_weights('models/item_rates_model_updated.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_test, predictions):\n",
    "  y_test= list(y_test)\n",
    "  predictions = list(predictions)\n",
    "  sum=0\n",
    "  for i in range(len(y_test)):\n",
    "    if y_test[i]==0:\n",
    "      continue\n",
    "    sum+=abs(((y_test[i]-predictions[i])/y_test[i]))\n",
    "  return (sum/len(y_test))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_label_classification_model(new_classification_data):\n",
    "  \n",
    "    text_corpus=list(data_used_to_train_label_classification_model[\"product_info\"])+new_classification_data[\"product_info\"]\n",
    "    labels=pd.Series(list(data_used_to_train_label_classification_model[\"label\"])+new_classification_data[\"label\"])\n",
    "    latest_updated_label_classifier_vectorizer = get_vectorizer(text_corpus=text_corpus, ngrams_upper_limit=2)\n",
    "\n",
    "    possible_labels=[\"trims\", \"description\", \"cmt\", \"packing\", \"fabric\"]\n",
    "    target_dict= {'cmt': 2, 'description': 1, 'fabric': 4, 'packing': 3, 'trims': 0}\n",
    "    #target_dict = {n:i for i, n in enumerate(possible_labels)}\n",
    "\n",
    "    y = labels.map(target_dict)   #.map only works for pandas Series and not list\n",
    "    y_cat = to_categorical(y)     #converting the lavels into binary vectors of 0s and 1s \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(latest_updated_label_classifier_vectorizer.transform(text_corpus).toarray().shape[1], ) , activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='tanh'))\n",
    "    model.add(Dense(128, activation='tanh'))\n",
    "    model.add(Dense(64, activation='tanh'))\n",
    "    model.add(Dense(5,  activation='softmax'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.00001)\n",
    "\n",
    "    model.compile(optimizer=adam, \n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    early_stopping_monitor = EarlyStopping(patience=200)\n",
    "    r=model.fit(latest_updated_label_classifier_vectorizer.transform(text_corpus).toarray(), y_cat, epochs=500, validation_split=0.2,callbacks=[early_stopping_monitor])\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/label_classification_model_updated.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"models/label_classification_model_updated.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    with open('vectorizers/label_classifier_vectorizer.pk', 'wb') as fin:\n",
    "        pickle.dump(latest_updated_label_classifier_vectorizer, fin)\n",
    "     \n",
    "    y_pred = model.predict(latest_updated_label_classifier_vectorizer.transform(text_corpus).toarray())\n",
    "    y_pred_class = convert_to_text_labels(np.argmax(y_pred, axis=1))\n",
    "    y_test_class = convert_to_text_labels(np.argmax(y_cat, axis=1))\n",
    "    \n",
    "    print(classification_report(y_test_class, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_item_rates(improved_item_rates):\n",
    "    text_corpus=[]\n",
    "    rates=[]\n",
    "    items=list(improved_item_rates.keys())\n",
    "    values=list(improved_item_rates.values())\n",
    "    for i in range(len(items)):\n",
    "        text_corpus+=[items[i]]*20\n",
    "        rates+=[values[i]]*20\n",
    "        \n",
    "    response= label_classifier_vectorizer.transform(text_corpus)\n",
    "    X=response.todense()\n",
    "    \n",
    "    Y=np.array(rates)\n",
    "   \n",
    "    model=Sequential()\n",
    "    \n",
    "    n_cols=X.shape[1]\n",
    "    \n",
    "    #add model layers\n",
    "    model.add(Dense(1024,  input_shape=(n_cols,)))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    #compile model using mse as a measure of model performance\n",
    "    adam = optimizers.Adam(lr=0.00001)\n",
    "    model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "    \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=50)\n",
    "    #train model\n",
    "    \n",
    "    r = model.fit(X, Y, validation_split=0.3, epochs=250, callbacks=[early_stopping_monitor])\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(\"models/item_rates_model_updated.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"models/item_rates_model_updated.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    predictions=model.predict(X)\n",
    "    predictions=predictions.reshape(len(predictions),)\n",
    "    \n",
    "    print(mape(rates, predictions))\n",
    "    print(rates[0])\n",
    "    print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_item_avgs(improved_item_avgs):\n",
    "    text_corpus=[]\n",
    "    avgs=[]\n",
    "    items=list(improved_item_avgs.keys())\n",
    "    values=list(improved_item_avgs.values())\n",
    "    for i in range(len(items)):\n",
    "        text_corpus+=[items[i]]*20\n",
    "        avgs+=[values[i]]*20\n",
    "        \n",
    "    response= label_classifier_vectorizer.transform(text_corpus)\n",
    "    X=response.todense()\n",
    "    \n",
    "    Y=np.array(avgs)\n",
    "   \n",
    "    model=Sequential()\n",
    "    \n",
    "    n_cols=X.shape[1]\n",
    "    \n",
    "    #add model layers\n",
    "    model.add(Dense(1024,  input_shape=(n_cols,)))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    #compile model using mse as a measure of model performance\n",
    "    adam = optimizers.Adam(lr=0.00001)\n",
    "    model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "    \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    #set early stopping monitor so the model stops training when it won't improve anymore\n",
    "    early_stopping_monitor = EarlyStopping(patience=50)\n",
    "    #train model\n",
    "    \n",
    "    r = model.fit(X, Y, validation_split=0.3, epochs=250, callbacks=[early_stopping_monitor])\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(\"models/item_avgs_model_updated.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"models/item_avgs_model_updated.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    predictions=model.predict(X)\n",
    "    predictions=predictions.reshape(len(predictions),)\n",
    "    \n",
    "    print(mape(avgs, predictions))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(current_dictionary, database_dictionary):\n",
    "    for key in current_dictionary.keys():\n",
    "        try:\n",
    "            database_dictionary[key]=round((float(database_dictionary[key])+float(current_dictionary[key]))/2,3)\n",
    "        except:\n",
    "            database_dictionary[key]=current_dictionary[key]\n",
    "    return  database_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 5, 'a': 2.5, 'd': 'lawda', 'b': 3, 'c': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " merge_dictionaries(current_dictionary={'a':2, 'b':3, 'c':4, 'd':\"lawda\"}, database_dictionary={'e':5, 'a':3, 'd':\"lund\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_beakup_text_from_dict(breakup_dictionary):\n",
    "    output = {}\n",
    "    clean_text=[]\n",
    "    underscored_text=[]\n",
    "    for key in breakup_dictionary.keys():\n",
    "        try:\n",
    "            for item in breakup_dictionary[key][\"items\"]:\n",
    "                clean_text.append(item[\"name\"])\n",
    "                underscored_text.append(\"_\".join(item[\"name\"].split()))\n",
    "        except:\n",
    "            if key==\"description\":\n",
    "                clean_text.append(breakup_dictionary[\"description\"])\n",
    "                underscored_text.append(\"_\".join(breakup_dictionary[\"description\"].split()))\n",
    "    output[\"space_seperated\"]=\" \".join(clean_text)\n",
    "    output[\"underscore_seperated\"]=\" \".join(underscored_text)\n",
    "    if \"buyer\" in breakup_dictionary.keys():\n",
    "        output[\"buyer\"]=breakup_dictionary[\"buyer\"]\n",
    "    else:\n",
    "        output[\"buyer\"]=\"\"\n",
    "        \n",
    "    if \"user\" in breakup_dictionary.keys():\n",
    "        output[\"user\"]=breakup_dictionary[\"user\"]\n",
    "    else:\n",
    "        output[\"user\"]=\"\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_labels_from_dict(nested_dict):\n",
    "    product_info=[]\n",
    "    label=[]\n",
    "    product_info.append(nested_dict[\"description\"])\n",
    "    label.append(\"description\")\n",
    "    \n",
    "    if \"buyer\" in nested_dict.keys() and len(nested_dict[\"buyer\"])>=2:\n",
    "        product_info.append(nested_dict[\"buyer\"])\n",
    "        label.append(\"buyer\")\n",
    "        \n",
    "    for key in nested_dict.keys():\n",
    "        if key in [\"description\",\"Total cost\", \"buyer\", \"user\"]:\n",
    "            continue\n",
    "        for item in nested_dict[key][\"items\"]:\n",
    "            product_info.append(item[\"name\"])\n",
    "            label.append(key)\n",
    "        \n",
    "    return {\"product_info\":product_info, \"label\":label}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_rates_from_dict(nested_dict):\n",
    "    complete_item_info=[]\n",
    "    rate=[]\n",
    "    \n",
    "    for key in nested_dict.keys():\n",
    "        try:\n",
    "            if key in [\"description\",\"Total cost\", \"buyer\", \"user\"]:\n",
    "                continue\n",
    "            if \"buyer\" in nested_dict.keys() and len(nested_dict[\"buyer\"])<2:\n",
    "                nested_dict[\"buyer\"]=\"\"\n",
    "            for item in nested_dict[key][\"items\"]:\n",
    "                complete_item_info.append((nested_dict[\"buyer\"]+\" \"+nested_dict[\"description\"]+\" \"+item[\"name\"]).strip())\n",
    "                rate.append(float(item[\"rate\"]))\n",
    "        except:\n",
    "            continue\n",
    "    if len(rate)==0:\n",
    "        print('No record entered from ', nested_dict, \" , probably some error in the item_rates_from_dict() function\")\n",
    "    return dict(zip(complete_item_info,rate))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uom_from_dict(nested_dict):\n",
    "    complete_item_info=[]\n",
    "    uom=[]\n",
    "    \n",
    "    for key in nested_dict.keys():\n",
    "        try:\n",
    "            if key in [\"description\",\"Total cost\", \"buyer\", \"user\"]:\n",
    "                continue\n",
    "            if \"buyer\" in nested_dict.keys() and len(nested_dict[\"buyer\"])<2:\n",
    "                nested_dict[\"buyer\"]=\"\"\n",
    "            for item in nested_dict[key][\"items\"]:\n",
    "                complete_item_info.append(item[\"name\"].strip())\n",
    "                uom.append(item[\"UOM\"])\n",
    "        except:\n",
    "            continue\n",
    "    if len(uom)==0:\n",
    "        print('No record entered from ', nested_dict, \" , probably some error in the uom_from_dict() function\")\n",
    "    return dict(zip(complete_item_info,uom))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_avgs_from_dict(nested_dict):\n",
    "    complete_item_info=[]\n",
    "    average_quantity=[]\n",
    "    \n",
    "    for key in nested_dict.keys():\n",
    "        try:\n",
    "            if key in [\"description\",\"Total cost\", \"buyer\", \"user\"]:\n",
    "                continue\n",
    "            if \"buyer\" in nested_dict.keys() and len(nested_dict[\"buyer\"])<2:\n",
    "                nested_dict[\"buyer\"]=\"\"\n",
    "            for item in nested_dict[key][\"items\"]:\n",
    "                complete_item_info.append((nested_dict[\"buyer\"]+\" \"+nested_dict[\"description\"]+\" \"+item[\"name\"]).strip())\n",
    "                average_quantity.append(float(str(item[\"avg\"]).split()[0]))\n",
    "        except:\n",
    "            continue\n",
    "    if len(average_quantity)==0:\n",
    "        print('No record entered from ', nested_dict, \" , probably some error in the item_avgs_from_dict() function\")\n",
    "    return dict(zip(complete_item_info,average_quantity))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NERmIng4Jp-M"
   },
   "source": [
    " # Creating vectorizer and prediction funtion for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymongo\n",
    "#!pip install flask_pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from flask import Flask\n",
    "from flask_pymongo import PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "costing_engine_db = client[\"costing_engine_db_with_login\"]   #Initialised/imported database named costing_engine_db\n",
    "user_improved_costing_collection=costing_engine_db[\"user_improved_costing_collection\"] #A collection/list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mongo_username_database(new_users,client=pymongo.MongoClient( \"mongodb://localhost:27017/\"), database_name=\"costing_engine_db_with_login\"):\n",
    "    try:\n",
    "        current_database=list(client[database_name][\"users\"].find({\"database_name\":\"username_password\"}))[0]\n",
    "        updated_database=current_database\n",
    "        updated_database.pop(\"_id\")\n",
    "        for key,value in new_users.items():\n",
    "            updated_database[key]=value\n",
    "        client[\"costing_engine_db_with_login\"][\"users\"].replace_one({\"database_name\":\"username_password\"}, updated_database)\n",
    "        print(\"username password database updated\")\n",
    "    except:\n",
    "        print(\"username password database couldn't be updated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the data from mongoDB database into variables and datastructures to be used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global improved_item_rates\n",
    "global improved_item_avgs\n",
    "global user_garment_text_description\n",
    "global new_label_classification_data\n",
    "global improved_item_breakup_database\n",
    "global uom\n",
    "improved_item_rates={}\n",
    "improved_item_avgs={}\n",
    "improved_item_breakup_database={}\n",
    "uom={}\n",
    "new_label_classification_data={\"product_info\":[], \"label\":[]}\n",
    "\n",
    "\n",
    "for improved_dict in user_improved_costing_collection.find():\n",
    "    ##### Populating improved_item_breakup_database dictionary with improved_dict values from momgo database collection#########\n",
    "    improved_dict.pop(\"_id\")\n",
    "    user_garment_text_description=improved_dict[\"user_garment_text_description\"]\n",
    "    improved_dict.pop(\"user_garment_text_description\")\n",
    "    user=improved_dict[\"username\"]\n",
    "    improved_dict.pop(\"username\")\n",
    "    if user not in improved_item_breakup_database.keys():\n",
    "        improved_item_breakup_database[user]={}\n",
    "    improved_item_breakup_database[user][user_garment_text_description]=item_beakup_text_from_dict(improved_dict)\n",
    "    \n",
    "    ################### Populating improved_item_rates/avgs dictionary####################################################\n",
    "    improved_item_rates=merge_dictionaries(current_dictionary=item_rates_from_dict(improved_dict), \n",
    "                                           database_dictionary=improved_item_rates)\n",
    "    improved_item_avgs=merge_dictionaries(current_dictionary=item_avgs_from_dict(improved_dict), \n",
    "                                           database_dictionary=improved_item_avgs)\n",
    "    uom=merge_dictionaries(current_dictionary=uom_from_dict(improved_dict), \n",
    "                                           database_dictionary=uom)\n",
    "    ######################################################################################################################\n",
    "    new_item_label_dict=item_labels_from_dict(improved_dict)\n",
    "    \n",
    "    ######## Finding new user entered items/products/terms which were not in database previously and \n",
    "    ###########################################################adding them to the new_label_classification_dictionary##########\n",
    "    for i in range(len(new_item_label_dict[\"product_info\"])):\n",
    "        if new_item_label_dict[\"product_info\"][i] not in new_label_classification_data[\"product_info\"]:\n",
    "            new_label_classification_data[\"product_info\"].append(new_item_label_dict[\"product_info\"][i])\n",
    "            new_label_classification_data[\"label\"].append(new_item_label_dict[\"label\"][i])\n",
    "    ###########################################################################################################################\n",
    "    \n",
    "################################## neural network retraining trigger #######################################################\n",
    "# if len(new_label_classification_data[\"product_info\"])>150:\n",
    "#     retrain_label_classification_model(new_label_classification_data)   \n",
    "############################################################################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Variables/arrays of texts used previously by these names#############################\n",
    "global underscored_corpus\n",
    "global space_clean_corpus \n",
    "\n",
    "underscored_corpus=[]\n",
    "space_clean_corpus = []\n",
    "\n",
    "for dic in list(improved_item_breakup_database.values()):\n",
    "    underscored_corpus.append(dic[\"underscore_seperated\"])\n",
    "    space_clean_corpus.append(dic[\"space_seperated\"])\n",
    "underscored_corpus=np.array(underscored_corpus)\n",
    "space_clean_corpus=np.array(space_clean_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_label_dict() function for converting any text into dictionary with labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_dict(text):\n",
    "\n",
    "  undefined_word_present = False                                            #Incase user enters some undefined words\n",
    "  correct_tokens_filled_in_dictionary_uptill_now = [\"description\", \"cmt\", \"packing\", \"fabric\", \"trims\", \"others\"]\n",
    "  text=text.lower()\n",
    "  text=text.strip()\n",
    "  tokens=re.findall(r\"[\\w']+\", text)\n",
    "  tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    " \n",
    "  label_dict={}\n",
    "  for token in tokens:\n",
    "    correct_token = \"\"\n",
    "    if \" \".join(token.split(\"_\")) in new_label_classification_data[\"product_info\"]:\n",
    "        correct_token=\" \".join(token.split(\"_\"))\n",
    "    else:\n",
    "        for word in token.split('_'):\n",
    "          if word not in list(label_classifier_vectorizer.vocabulary_.keys()):\n",
    "            lemmatized_word =wordnet_lemmatizer.lemmatize(word, pos=\"v\")\n",
    "            if lemmatized_word not in list(label_classifier_vectorizer.vocabulary_.keys()):\n",
    "              correct_spell_word=str(TextBlob(word).correct())\n",
    "              if correct_spell_word not in list(label_classifier_vectorizer.vocabulary_.keys()):\n",
    "                stemmed_word = nltk.PorterStemmer().stem(word)\n",
    "                if stemmed_word not in list(label_classifier_vectorizer.vocabulary_.keys()):\n",
    "                  if not undefined_word_present:\n",
    "                    undefined_word_present = word\n",
    "                  else:\n",
    "                    undefined_word_present+=\" \"+word\n",
    "                  continue\n",
    "                else:\n",
    "                  word= stemmed_word\n",
    "              else:\n",
    "                word = correct_spell_word\n",
    "            else:\n",
    "              word= lemmatized_word\n",
    "          correct_token+=\" \"+word\n",
    "    \n",
    "    correct_token=correct_token.strip()\n",
    " \n",
    "    \n",
    "    if \" \".join(token.split(\"_\")) in new_label_classification_data[\"product_info\"]:\n",
    "        correct_token=\" \".join(token.split(\"_\"))\n",
    "    elif correct_token==\"\" or correct_token not in list(label_classifier_vectorizer.vocabulary_.keys()) :\n",
    "      continue\n",
    "        \n",
    "        \n",
    "    current_label=predict_label(correct_token)\n",
    "    if (current_label not in label_dict.keys()) and (correct_token.strip() not in correct_tokens_filled_in_dictionary_uptill_now):\n",
    "      label_dict[current_label]=correct_token\n",
    "      correct_tokens_filled_in_dictionary_uptill_now.append(correct_token)\n",
    "    else:\n",
    "      if (current_label in label_dict.keys()) and (correct_token.strip() not in label_dict[current_label]) and (correct_token.strip() not in correct_tokens_filled_in_dictionary_uptill_now):\n",
    "        label_dict[current_label]+=' , '+correct_token\n",
    "        correct_tokens_filled_in_dictionary_uptill_now.append(correct_token.strip())\n",
    "    if current_label in label_dict.keys():\n",
    "      label_dict[current_label]= label_dict[current_label].strip()\n",
    "    \n",
    "#   if \"fabric\" in label_dict and \"yarn\" in label_dict[\"fabric\"]:\n",
    "#     print(label_dict['fabric'])\n",
    "#     label_dict[\"fabric\"]=label_dict[\"fabric\"].replace(\", dyed\",\"\")\n",
    "#     label_dict[\"fabric\"]=label_dict[\"fabric\"].replace(\"yarn\",\"yarn dyed\")\n",
    "#     label_dict[\"fabric\"]=label_dict[\"fabric\"].strip()\n",
    "  return label_dict, undefined_word_present\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmQeXkk_J9KO"
   },
   "source": [
    "## Label prediction model function: predict_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqX7XCvaKDuL"
   },
   "outputs": [],
   "source": [
    "def invert_dict_class_weights(target_dict):\n",
    "  inverted_dict={}\n",
    "  class_weights={}\n",
    "  for item in target_dict.items():\n",
    "    inverted_dict[item[1]]= str(item[0])\n",
    "  \n",
    "  return inverted_dict\n",
    "result_dict= invert_dict_class_weights(target_dict)\n",
    "\n",
    "\n",
    "def convert_to_text_labels(y):\n",
    "  y = list(y)\n",
    "  for i in range(len(y)):\n",
    "    y[i]= result_dict[y[i]]\n",
    "\n",
    "  return y\n",
    "\n",
    "def predict_label(product_name):\n",
    "  product_name=product_name.lower()\n",
    "  if product_name in new_label_classification_data[\"product_info\"]:\n",
    "        return new_label_classification_data[\"label\"][new_label_classification_data[\"product_info\"].index(product_name)]\n",
    "\n",
    "  product_name = [product_name.lower()]\n",
    "  y_pred = label_classification_model.predict(label_classifier_vectorizer.transform(product_name).toarray())\n",
    "  y_pred_class = convert_to_text_labels(np.argmax(y_pred, axis=1))\n",
    "  return y_pred_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1762,
     "status": "ok",
     "timestamp": 1566068022328,
     "user": {
      "displayName": "Pranjal upadhyay",
      "photoUrl": "",
      "userId": "06624524539236247766"
     },
     "user_tz": -330
    },
    "id": "pt2gbwzPKUqm",
    "outputId": "9a5c7718-8a73-40a8-e367-1b72cfdcdcdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label(\"saree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "#comparison_vectorizer_corpus=list(improved_item_rates.keys())+list(data_used_to_train_rate_quantity_model[\"complete_item_info\"])\n",
    "comparison_vectorizer_corpus=list(improved_item_rates.keys()) if len(improved_item_rates.keys())>0 else [\"empty\"]\n",
    "comparison_vectorizer=TfidfVectorizer()\n",
    "comparison_vectorizer.fit(comparison_vectorizer_corpus) \n",
    "\n",
    "def text_similarity(text1, text2):\n",
    "    text1_vector = comparison_vectorizer.transform([text1.lower()])\n",
    "    text2_vector=comparison_vectorizer.transform([text2.lower()])\n",
    "    cosine_similarities = linear_kernel(text1_vector, text2_vector).flatten()\n",
    "    return cosine_similarities[0]\n",
    "\n",
    "def most_similar_key_for_text_from_dict(text,database_dict):\n",
    "    if len(database_dict.keys())==0:\n",
    "        return False\n",
    "    text=text.lower()\n",
    "    key_corpus=np.array(list(database_dict.keys()))\n",
    "    cosine_similarities = linear_kernel(comparison_vectorizer.transform([text]),\n",
    "                                        comparison_vectorizer.transform(key_corpus)).flatten()\n",
    "    most_similar_key = key_corpus[cosine_similarities.argsort()[::-1]][0]\n",
    "    \n",
    "    text_dict,_= get_label_dict(text)\n",
    "    most_similar_key_dict,_=get_label_dict(most_similar_key)\n",
    "    i=0\n",
    "    while i<len(key_corpus) and i<10:\n",
    "        most_similar_key = key_corpus[cosine_similarities.argsort()[::-1]][i] \n",
    "        most_similar_key_dict,_=get_label_dict(most_similar_key)\n",
    "        \n",
    "        if (\"description\" in text_dict.keys()) and (\"description\" in most_similar_key_dict.keys()) and text_similarity(most_similar_key, text)>0.8:\n",
    "            len_descriptions_intersection=len(set([x.strip() for x in text_dict[\"description\"].split(',')]).intersection(set([x.strip() for x in most_similar_key_dict[\"description\"].split(',')])))\n",
    "            if len_descriptions_intersection>0:\n",
    "                print(\"USED KEY: (\",most_similar_key+\")--\"+text+\"--\"+\" \"+str(text_similarity(most_similar_key, text)))\n",
    "                return most_similar_key\n",
    "        elif (\"description\" not in text_dict.keys()) and (\"description\" not in most_similar_key_dict.keys()) and text_similarity(most_similar_key, text)>0.65:\n",
    "            print(\"USED KEY: (\",most_similar_key+\")--\"+text+\"--\"+\" \"+str(text_similarity(most_similar_key, text)))\n",
    "            return most_similar_key\n",
    "        i+=1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_similar_key_for_text_from_dict(\"sleeves shirt computer embroidery\",{\"dress embroidery thread button silk\":1, \"shirt button thread embroidery\":2,\n",
    "#                                                                            \"embroidery thread button silk\":3, \n",
    "#                                                                            \"half sleeves shirt computer embroidery \":4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZTRbRQxKzoO"
   },
   "source": [
    "## Fabric quantity prediction function : predict_fabric_quantity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOMOY49IMMlf"
   },
   "outputs": [],
   "source": [
    "def predict_fabric_quantity(st):\n",
    "  if st in improved_item_avgs.keys():\n",
    "    return float(improved_item_avgs[st])\n",
    "  most_similar_key_to_st=most_similar_key_for_text_from_dict(st, improved_item_avgs )\n",
    "  if most_similar_key_to_st:\n",
    "    return improved_item_avgs[most_similar_key_to_st]\n",
    "  \n",
    "    \n",
    "  st = st.lower()\n",
    "  st_vector= label_classifier_vectorizer.transform([st]).todense()\n",
    "  predictions = item_avgs_model.predict(st_vector).reshape(1,)\n",
    "  \n",
    "                                                                                          \n",
    "  return predictions[0] if predictions[0]>0 else 1                                                                                                \n",
    "                                                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cksltoXVTwl"
   },
   "source": [
    "## Fabric rate prediction : predict_fabric_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DcJTlA7ZWX_N"
   },
   "outputs": [],
   "source": [
    "def predict_fabric_rate(st):\n",
    "  st = st.lower()\n",
    "  if st in improved_item_rates.keys():\n",
    "     return float(improved_item_rates[st])\n",
    "  most_similar_key_to_st=most_similar_key_for_text_from_dict(st, improved_item_rates)\n",
    "  if most_similar_key_to_st:\n",
    "    return improved_item_rates[most_similar_key_to_st]\n",
    "  \n",
    "  st_vector= label_classifier_vectorizer.transform([st]).todense()\n",
    "  predictions = item_rates_model.predict(st_vector)\n",
    "  predictions= predictions.reshape(len(predictions),)   \n",
    "                                                                                               \n",
    "  return predictions[0] if predictions[0]>0 else 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.009544"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fabric_rate(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRz8v5UvYO7H"
   },
   "source": [
    "## Trim Quantity prediction function : predict_trim_quantity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4n4XzEbZkmX"
   },
   "outputs": [],
   "source": [
    "def predict_trim_quantity(st):\n",
    "  if st in improved_item_avgs.keys():\n",
    "    return improved_item_avgs[st]\n",
    "  most_similar_key_to_st=most_similar_key_for_text_from_dict(st, improved_item_avgs )\n",
    "  if most_similar_key_to_st:\n",
    "    return improved_item_avgs[most_similar_key_to_st]\n",
    "\n",
    "  st = st.lower()\n",
    "  st_vector= label_classifier_vectorizer.transform([st]).todense()\n",
    "  predictions = item_avgs_model.predict(st_vector).reshape(1,)\n",
    "                                                                                        \n",
    "  return predictions[0] if predictions[0]>0 else 1           \n",
    "                                                                                              \n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EPN-EEFhmuN"
   },
   "source": [
    "## Trims rate prediction function : predict_trim_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PxjKddWjcO-"
   },
   "outputs": [],
   "source": [
    "def predict_trim_rate(st):\n",
    "  if st in improved_item_rates.keys():\n",
    "    return improved_item_rates[st]\n",
    "  most_similar_key_to_st=most_similar_key_for_text_from_dict(st, improved_item_rates )\n",
    "  print(most_similar_key_to_st)\n",
    "  if most_similar_key_to_st:\n",
    "    return improved_item_rates[most_similar_key_to_st]\n",
    "\n",
    "  st = st.lower()\n",
    "\n",
    "  st_vector=label_classifier_vectorizer.transform([st]).todense()\n",
    "  predictions=item_rates_model.predict(st_vector).reshape(1,)\n",
    "       \n",
    "  return predictions[0] if predictions[0]>0 else 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35.009544"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_trim_rate(\"khkhl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGScdhKHkC1w"
   },
   "source": [
    "## Processes rates prediction function: predict_process_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUsX5YDdk_hZ"
   },
   "outputs": [],
   "source": [
    "def predict_process_rate(st):\n",
    "  if st in improved_item_rates.keys():\n",
    "    print(\"Found exact key in improved_item_rates dictionary\")\n",
    "    return improved_item_rates[st]\n",
    "  most_similar_key_to_st=most_similar_key_for_text_from_dict(st, improved_item_rates )\n",
    "  if most_similar_key_to_st:\n",
    "    print(\"Found similar key in improved_item_rates dictionary: \", most_similar_key_to_st, \" for item: \",st, \" in  predict_process_rate()\")\n",
    "    return improved_item_rates[most_similar_key_to_st]\n",
    "  print(\"Most similar key: \", most_similar_key_to_st)\n",
    "    \n",
    "  #If not in database, predict with neural network\n",
    "  st = st.lower()\n",
    "  \n",
    "  st_vector=label_classifier_vectorizer.transform([st]).todense()\n",
    "  predictions=item_rates_model.predict(st_vector).reshape(1,)\n",
    "       \n",
    "  return predictions[0] if predictions[0]>0 else 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## Function to get most similar texts to the user input #########################################################################\n",
    "\n",
    "def get_multiple_similar_text_from_database(user_input_text, n,quotation_len_minimum_threshold ):\n",
    "  global underscored_corpus\n",
    "  global space_clean_corpus \n",
    "  user_input_dict,_= get_label_dict(user_input_text)\n",
    "\n",
    "  \n",
    "  output = {\n",
    "      \"space_seperated\":[],\n",
    "      \"underscore_seperated\":[] \n",
    "      }\n",
    "\n",
    "  if \"description\" in user_input_dict.keys():\n",
    "    user_input_dict[\"description\"]=\" \".join([x.strip() for x in user_input_dict[\"description\"].split(',')])\n",
    "    corrected_user_input = \" \".join(user_input_dict[\"description\"].split(\" , \"))\n",
    "    for word in user_input_text.split():\n",
    "        if word not in corrected_user_input:\n",
    "            corrected_user_input+=\" \"+word\n",
    "    user_input_text=corrected_user_input\n",
    "\n",
    "  else:\n",
    "    return output\n",
    "  try:\n",
    "      clean_text_vectorizer = TfidfVectorizer(ngram_range=(1,5))\n",
    "      response = clean_text_vectorizer.fit_transform(list(space_clean_corpus))\n",
    "  except: \n",
    "      print(\"Empty space clean vectorizer.Cannot fit_transform vectorizer on empty text corpus in the get_multiple_similar_text_from_database() function\")\n",
    "      return output\n",
    "\n",
    "  input_text_vector = clean_text_vectorizer.transform([user_input_text])\n",
    "  cosine_similarities = linear_kernel(input_text_vector, response).flatten()\n",
    "  related_docs_indices = cosine_similarities.argsort()[:-1*(n+1):-1]\n",
    "  clean_texts = list(space_clean_corpus[related_docs_indices])\n",
    "  underscored_texts = list(underscored_corpus[related_docs_indices])\n",
    " \n",
    "#   quotation_len_minimum_threshold=15\n",
    "  while len(output[\"space_seperated\"])==0 and quotation_len_minimum_threshold>0:\n",
    "      output = {\n",
    "      \"space_seperated\":[],\n",
    "      \"underscore_seperated\":[] \n",
    "      }\n",
    "      for i in range(len(clean_texts)):\n",
    "        clean_text=re.sub('[^A-Za-z0-9_]+', ' ',clean_texts[i])\n",
    "        underscored_text= re.sub('[^A-Za-z0-9_]+', ' ',underscored_texts[i])\n",
    "        if len(set(user_input_dict[\"description\"].split()).intersection(set(clean_text.split())))!=0 and len(underscored_text.split())>quotation_len_minimum_threshold :\n",
    "            output[\"space_seperated\"].append(clean_text.strip())\n",
    "            output[\"underscore_seperated\"].append(underscored_text.strip())\n",
    "      quotation_len_minimum_threshold=quotation_len_minimum_threshold-1\n",
    "  return output\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_multiple_similar_text_from_database(\"saree\", 1, 1)['underscore_seperated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fabric(user_txt, quotation_len_minimum_threshold, batch_size):\n",
    "    try:\n",
    "        predicted_fabrics = False\n",
    "        fabric_prediction_quotation_text=\"NONE\"\n",
    "        while not predicted_fabrics:\n",
    "            if quotation_len_minimum_threshold>=50:\n",
    "                return predicted_fabrics\n",
    "            fabric_prediction_quotation_text = get_multiple_similar_text_from_database(user_txt,\n",
    "                                                        batch_size, \n",
    "                                                        quotation_len_minimum_threshold)[\"underscore_seperated\"][0]\n",
    "            fabric_prediction_dict, _= get_label_dict(fabric_prediction_quotation_text)\n",
    "            #print(fabric_prediction_quotation_text)\n",
    "            if \"fabric\" in fabric_prediction_dict.keys():\n",
    "                predicted_fabrics=fabric_prediction_dict[\"fabric\"].split(\" , \")\n",
    "            quotation_len_minimum_threshold+=1\n",
    "\n",
    "        incomplete_dict_from_user_input, undefined_word_present=get_label_dict(user_txt)\n",
    "        if \"fabric\" not in incomplete_dict_from_user_input.keys():\n",
    "            return predicted_fabrics,  fabric_prediction_quotation_text\n",
    "        for fabric in incomplete_dict_from_user_input[\"fabric\"].split(\" , \"):\n",
    "            if fabric not in \" \".join(predicted_fabrics):\n",
    "                predicted_fabrics.append(fabric)\n",
    "        return predicted_fabrics, fabric_prediction_quotation_text\n",
    "    except:\n",
    "        print(\"Empty database or some error in predict_fabric() function\")\n",
    "        return [],\"empty\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty space clean vectorizer.Cannot fit_transform vectorizer on empty text corpus in the get_multiple_similar_text_from_database() function\n",
      "Empty database or some error in predict_fabric() function\n",
      "fabrics predicted:  []\n",
      "Quotation text used :  empty\n"
     ]
    }
   ],
   "source": [
    "fabrics, fabric_text=predict_fabric(\"nylon shirt embroidery and threads\", quotation_len_minimum_threshold=3, batch_size=200)\n",
    "print(\"fabrics predicted: \", fabrics)\n",
    "print(\"Quotation text used : \",fabric_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trims(user_txt, quotation_len_minimum_threshold, batch_size):\n",
    "    try:\n",
    "        predicted_trims = False\n",
    "        trims_prediction_quotation_text=\"NONE\"\n",
    "        while not predicted_trims:\n",
    "            if quotation_len_minimum_threshold>=50:\n",
    "                return predicted_trims\n",
    "            trims_prediction_quotation_text = get_multiple_similar_text_from_database(user_txt,\n",
    "                                                        batch_size, \n",
    "                                                        quotation_len_minimum_threshold)[\"underscore_seperated\"][0]\n",
    "            trims_prediction_dict, _= get_label_dict(trims_prediction_quotation_text)\n",
    "            #print(trims_prediction_quotation_text)\n",
    "            if \"trims\" in trims_prediction_dict.keys():\n",
    "                predicted_trims=trims_prediction_dict[\"trims\"].split(\" , \")\n",
    "            quotation_len_minimum_threshold+=1\n",
    "\n",
    "        incomplete_dict_from_user_input, undefined_word_present=get_label_dict(user_txt)\n",
    "        if \"trims\" not in incomplete_dict_from_user_input.keys():\n",
    "            return predicted_trims, trims_prediction_quotation_text\n",
    "        for trim in incomplete_dict_from_user_input[\"trims\"].split(\" , \"):\n",
    "            if trim not in \" \".join(predicted_trims):\n",
    "                predicted_trims.append(trim)\n",
    "        return predicted_trims, trims_prediction_quotation_text\n",
    "    except:\n",
    "        print(\"Empty database or some error in predict_fabric() function\")\n",
    "        return [],\"empty\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty space clean vectorizer.Cannot fit_transform vectorizer on empty text corpus in the get_multiple_similar_text_from_database() function\n",
      "Empty database or some error in predict_fabric() function\n",
      "trims predicted:  []\n",
      "Quotation text used :  empty\n"
     ]
    }
   ],
   "source": [
    "trims, trim_text=predict_trims(\"silk blouse\", quotation_len_minimum_threshold=8, batch_size=200)\n",
    "print(\"trims predicted: \", trims)\n",
    "print(\"Quotation text used : \",trim_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cmt(user_txt, quotation_len_minimum_threshold, batch_size):\n",
    "    try:\n",
    "        predicted_cmt = False\n",
    "        cmt_prediction_quotation_text=\"NONE\"\n",
    "        while not predicted_cmt:\n",
    "            if quotation_len_minimum_threshold>=50:\n",
    "                return predicted_cmt\n",
    "            cmt_prediction_quotation_text = get_multiple_similar_text_from_database(user_txt,\n",
    "                                                        batch_size, \n",
    "                                                        quotation_len_minimum_threshold)[\"underscore_seperated\"][0]\n",
    "            cmt_prediction_dict, _= get_label_dict(cmt_prediction_quotation_text)\n",
    "            #print(cmt_prediction_quotation_text)\n",
    "            if \"cmt\" in cmt_prediction_dict.keys():\n",
    "                predicted_cmt=cmt_prediction_dict[\"cmt\"].split(\" , \")\n",
    "            quotation_len_minimum_threshold+=1\n",
    "\n",
    "        incomplete_dict_from_user_input, undefined_word_present=get_label_dict(user_txt)\n",
    "        if \"cmt\" not in incomplete_dict_from_user_input.keys():\n",
    "            return predicted_cmt, cmt_prediction_quotation_text\n",
    "        for cmt in incomplete_dict_from_user_input[\"cmt\"].split(\" , \"):\n",
    "            if cmt not in \" \".join(predicted_cmt):\n",
    "                predicted_cmt.append(cmt)\n",
    "        return predicted_cmt, cmt_prediction_quotation_text\n",
    "    except:\n",
    "        print(\"Empty database or some error in predict_fabric() function\")\n",
    "        return [],\"empty\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty space clean vectorizer.Cannot fit_transform vectorizer on empty text corpus in the get_multiple_similar_text_from_database() function\n",
      "Empty database or some error in predict_fabric() function\n",
      "cmt predicted:  []\n",
      "Quotation text used :  empty\n"
     ]
    }
   ],
   "source": [
    "cmt, cmt_text=predict_cmt(\"shirt\", quotation_len_minimum_threshold=9, batch_size=200)\n",
    "print(\"cmt predicted: \", cmt)\n",
    "print(\"Quotation text used : \",cmt_text)\n",
    "# print(\"Quotation_number/Order number: \",quotation_list[orignal_text_list.index(cmt_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_packing(user_txt, quotation_len_minimum_threshold, batch_size):\n",
    "    try:\n",
    "        predicted_packing = False\n",
    "        packing_prediction_quotation_text=\"NONE\"\n",
    "        while not predicted_packing:\n",
    "            if quotation_len_minimum_threshold>=50:\n",
    "                return predicted_packing\n",
    "            packing_prediction_quotation_text = get_multiple_similar_text_from_database(user_txt,\n",
    "                                                        batch_size, \n",
    "                                                        quotation_len_minimum_threshold)[\"underscore_seperated\"][0]\n",
    "            packing_prediction_dict, _= get_label_dict(packing_prediction_quotation_text)\n",
    "            #print(packing_prediction_quotation_text)\n",
    "            if \"packing\" in packing_prediction_dict.keys():\n",
    "                predicted_packing=packing_prediction_dict[\"packing\"].split(\" , \")\n",
    "            quotation_len_minimum_threshold+=1\n",
    "\n",
    "        incomplete_dict_from_user_input, undefined_word_present=get_label_dict(user_txt)\n",
    "        if \"packing\" not in incomplete_dict_from_user_input.keys():\n",
    "            return predicted_packing, packing_prediction_quotation_text\n",
    "        for packing in incomplete_dict_from_user_input[\"packing\"].split(\" , \"):\n",
    "            if packing not in \" \".join(predicted_packing):\n",
    "                predicted_packing.append(packing)\n",
    "        return predicted_packing, packing_prediction_quotation_text\n",
    "    except:\n",
    "        print(\"Empty database or some error in predict_fabric() function\")\n",
    "        return [],\"empty\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty space clean vectorizer.Cannot fit_transform vectorizer on empty text corpus in the get_multiple_similar_text_from_database() function\n",
      "Empty database or some error in predict_fabric() function\n",
      "packing predicted:  []\n",
      "Quotation text used :  empty\n"
     ]
    }
   ],
   "source": [
    "packing, packing_text=predict_packing(\"silk blouse\", quotation_len_minimum_threshold=15, batch_size=200)\n",
    "print(\"packing predicted: \", packing)\n",
    "print(\"Quotation text used : \",packing_text)\n",
    "# print(\"Quotation_number/Order number: \",quotation_list[orignal_text_list.index(packing_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################### Version2 ##########################################################################################\n",
    "def get_item_breakup_dictionary_v4(user_txt):\n",
    "  \n",
    "  user_txt=user_txt.split()\n",
    "  for word in user_txt:\n",
    "    if word not in label_classifier_vectorizer.vocabulary_.keys() and word not in new_label_classification_data[\"product_info\"]:\n",
    "        user_txt.remove(word)\n",
    "  user_txt=\" \".join(user_txt)\n",
    "\n",
    "  incomplete_dict_from_user_input, undefined_word_present=get_label_dict(user_txt)\n",
    "\n",
    "  if \"description\" not in incomplete_dict_from_user_input.keys():\n",
    "    if undefined_word_present!=False:\n",
    "      print (\"Undeined entities: \"+undefined_word_present)\n",
    "      print(\"Not in database\")\n",
    "    return False\n",
    "  \n",
    "  incomplete_dict_from_user_input[\"description\"]=\" \".join([x.strip() for x in incomplete_dict_from_user_input[\"description\"].split(',')])\n",
    "  if set(incomplete_dict_from_user_input[\"description\"].split()).intersection(set(list(label_classifier_vectorizer.vocabulary_.keys())))==0:\n",
    "    print(\"Unknown description or new garment entered\")\n",
    "    return False\n",
    "    \n",
    "  user_txt=\"\"\n",
    "  for key in incomplete_dict_from_user_input.keys():\n",
    "    user_txt  += \" \"+incomplete_dict_from_user_input[key]   #Modifying the user input to the values of incomplete_dic_from_user_input\n",
    "    \n",
    "  user_txt=user_txt.strip()\n",
    "\n",
    "  \n",
    "  batch_size=200\n",
    "  \n",
    "  most_likely_texts_dictionary=get_multiple_similar_text_from_database(user_txt, batch_size, quotation_len_minimum_threshold=8)\n",
    "  most_likely_underscore_seperated_texts= most_likely_texts_dictionary[\"underscore_seperated\"]\n",
    "  most_likely_space_seperated_texts = most_likely_texts_dictionary[\"space_seperated\"]\n",
    "  space_clean_paragraph_words = (\" \".join(most_likely_space_seperated_texts)).split()\n",
    "\n",
    "  if len(most_likely_underscore_seperated_texts)==0:\n",
    "        print(\"Nothing in database\")\n",
    "        return False\n",
    "  \n",
    "  output_dict=incomplete_dict_from_user_input \n",
    "  \n",
    "\n",
    "  try:\n",
    "      predicted_fabrics =predict_fabric(user_txt, quotation_len_minimum_threshold=8, batch_size=batch_size)[0]\n",
    "      predicted_trims =predict_trims(user_txt, quotation_len_minimum_threshold=10, batch_size=batch_size)[0]\n",
    "      predicted_cmt =predict_cmt(user_txt, quotation_len_minimum_threshold=9, batch_size=batch_size)[0]\n",
    "      predicted_packing =predict_packing(user_txt, quotation_len_minimum_threshold=20, batch_size=batch_size)[0]\n",
    " \n",
    "      predictions={\"fabric\":predicted_fabrics, \"trims\":predicted_trims, \"cmt\":predicted_cmt,\"packing\":predicted_packing}\n",
    "  \n",
    "      for key in predictions.keys():\n",
    "        if not predictions[key]:\n",
    "            continue\n",
    "   \n",
    "        output_dict[key]=\" , \".join(predictions[key])\n",
    "      \n",
    "  except:\n",
    "    most_matched_text_with_user_input=most_likely_texts_dictionary[\"underscore_seperated\"][0].split()\n",
    "    for key in incomplete_dict_from_user_input.keys():\n",
    "        if incomplete_dict_from_user_input[key] not in most_matched_text_with_user_input:\n",
    "            most_matched_text_with_user_input.append(incomplete_dict_from_user_input[key])\n",
    "    most_matched_text_with_user_input=\" \".join(most_matched_text_with_user_input)   \n",
    "    output_dict,_=get_label_dict(most_matched_text_with_user_input)\n",
    "  output_dict[\"description\"]=\" \".join(incomplete_dict_from_user_input[\"description\"].split(\" , \"))\n",
    "  \n",
    "  return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_item_breakup_dictionary_v4(\"saree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fabric': 'nylon',\n",
       "  'description': 'shirt',\n",
       "  'cmt': 'embroidery',\n",
       "  'trims': 'thread'},\n",
       " False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_dict(\"nylon shirt embroidery and threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_likely_texts_dictionary=get_multiple_similar_text_from_database(\"blouse\",2, 200)\n",
    "# most_likely_underscore_seperated_texts= most_likely_texts_dictionary[\"underscore_seperated\"]\n",
    "# current_dict, undefined_word_present = get_label_dict((most_likely_underscore_seperated_texts[0]))\n",
    "# current_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7vSM7bknHve"
   },
   "outputs": [],
   "source": [
    "def dummy_output_dict(user_input_text):\n",
    "  item_breakup_dict, new_description=get_label_dict(user_input_text)\n",
    "  if \"description\" in item_breakup_dict.keys() and new_description:\n",
    "      item_breakup_dict[\"description\"]=new_description + \" \"+ item_breakup_dict[\"description\"]\n",
    "  elif \"description\" not in item_breakup_dict.keys() and  new_description:\n",
    "      item_breakup_dict[\"description\"]= new_description\n",
    "  elif \"description\" not in item_breakup_dict.keys() and  not new_description:\n",
    "      item_breakup_dict[\"description\"]=\"NA\"\n",
    "  #print(user_txt)\n",
    "  if \"fabric\" not in item_breakup_dict.keys(): \n",
    "    item_breakup_dict[\"fabric\"]=\"-\"\n",
    "  if \"trims\" not in item_breakup_dict.keys():\n",
    "    item_breakup_dict[\"trims\"]=\"-\"\n",
    "  if \"cmt\" not in item_breakup_dict.keys():\n",
    "    item_breakup_dict[\"cmt\"]=\"-\"\n",
    "  if \"packing\" not in item_breakup_dict.keys():\n",
    "    item_breakup_dict[\"packing\"]=\"-\"   \n",
    "    \n",
    "  output_dict= item_breakup_dict\n",
    "  for key in item_breakup_dict.keys():\n",
    "    if key==\"description\":\n",
    "        output_dict[key]=\" \".join([x.strip() for x in item_breakup_dict[key].split(',')])\n",
    "        continue\n",
    "    if key==\"fabric\":\n",
    "      \n",
    "      used_fabrics=item_breakup_dict[key].split(',')\n",
    "      output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for fabric in used_fabrics:\n",
    "        output_dict[key][\"items\"].append({\"name\":fabric.strip(),\n",
    "                                          \"avg\": 0,\n",
    "                                          \"rate\": 0,\n",
    "                                          \"amount\": 0,\n",
    "                                          \"uom\":\"\"\n",
    "                                          })\n",
    "      output_dict[key][\"total_cost\"]=0\n",
    "   \n",
    "    elif key ==\"trims\":\n",
    "      \n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "       \n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": 0,\n",
    "          \"rate\": 0,\n",
    "          \"amount\": 0,\n",
    "          \"uom\":\"\"\n",
    "        })\n",
    "        \n",
    "      output_dict[key][\"total_cost\"]=0\n",
    "    elif key ==\"packing\":\n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "        \n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": 0,\n",
    "          \"rate\": 0,\n",
    "          \"amount\": 0,\n",
    "          \"uom\":\"\"\n",
    "        })\n",
    "      output_dict[key][\"total_cost\"]=0\n",
    "    else:\n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "       \n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": 0,\n",
    "          \"rate\":0,\n",
    "          \"amount\": 0,\n",
    "          \"uom\":\"\"\n",
    "        })\n",
    "      output_dict[key][\"total_cost\"]=0\n",
    "  output_dict[\"Total cost\"] = 0\n",
    "  output_dict[\"buyer\"] = \"\"\n",
    "  return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHxQiGfUFShT"
   },
   "outputs": [],
   "source": [
    "def print_nested_dictionary(nested_dict):\n",
    "  for key in nested_dict.keys():\n",
    "    if type(nested_dict[key])==dict:\n",
    "      print( key.upper()+\" :\")\n",
    "      for k in nested_dict[key].keys():\n",
    "        if type(nested_dict[key][k]==dict):\n",
    "          print (\"    \"+k+\" :\")\n",
    "          for i in nested_dict[key][k].keys():\n",
    "            if type(nested_dict[key][k][i])==dict:\n",
    "              print(\"       \"+i + \" : \")\n",
    "              for j in nested_dict[key][k][i].keys():\n",
    "                print(\"           \"+j+\": \"+str(nested_dict[key][k][i][j]))\n",
    "            else:\n",
    "              print(\"       \"+i+\": \"+str(nested_dict[key][k][i]))\n",
    "        else:\n",
    "          print(\"   \"+k+\" : \"+nested_dict[key][k])\n",
    "          print()\n",
    "    else:\n",
    "      print(key.upper()+\" : \"+str(nested_dict[key]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_dict_with_cost_from_item_breakup_dic(nested_dict, buyer=\"\"):\n",
    "  item_breakup_dict = nested_dict\n",
    "  print(nested_dict)\n",
    "  if not item_breakup_dict:\n",
    "    return False\n",
    "  user_txt = item_breakup_dict[\"description\"]\n",
    "  #print(user_txt)\n",
    "  total_cost=0\n",
    "  output_dict={}\n",
    "  buyer=\"\" if len(buyer)<2 else buyer\n",
    "  for key in item_breakup_dict.keys():\n",
    "    if key==\"fabric\":\n",
    "      total_fabric_quantity=0\n",
    "      total_fabric_cost=0\n",
    "      used_fabrics=item_breakup_dict[key].split(',')\n",
    "      for fabric in used_fabrics:\n",
    "        fabric_quantity = float(predict_fabric_quantity((buyer+\" \"+user_txt+ \" \"+fabric).strip() ))\n",
    "        total_fabric_quantity+=fabric_quantity\n",
    "        \n",
    "      if \"fabric\" not in output_dict.keys():\n",
    "        output_dict[\"fabric\"]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for fabric in used_fabrics:\n",
    "        fabric_quantity = predict_fabric_quantity((buyer+\" \"+user_txt+ \" \"+fabric).strip())\n",
    "        print((buyer+\" \"+user_txt+ \" \"+fabric).strip(), \" HERE \", fabric_quantity)\n",
    "        if total_fabric_quantity>2.5 and (buyer+\" \"+user_txt+ \" \"+fabric).strip() not in improved_item_avgs.keys():\n",
    "            fabric_quantity= (fabric_quantity/total_fabric_quantity)*2.5\n",
    "        fabric_price_rate = predict_fabric_rate((buyer+\" \"+user_txt+ \" \"+fabric).strip())\n",
    "        print(\"Fabric rate for: \",buyer+\" \"+user_txt+ \" \"+fabric, \" :\", fabric_price_rate)\n",
    "        \n",
    "        total_cost+= fabric_quantity*fabric_price_rate\n",
    "        total_fabric_cost+=fabric_quantity*fabric_price_rate\n",
    "        output_dict[key][\"items\"].append({\"name\":fabric.strip(),\n",
    "                                          \"avg\": str(round(fabric_quantity,2))+\" meters\",\n",
    "                                          \"rate\": round(fabric_price_rate,3),\n",
    "                                          \"amount\": round(fabric_quantity*fabric_price_rate,2),\n",
    "                                          \"uom\": uom[fabric.strip()] if fabric.strip() in uom.keys() else \"\"\n",
    "                                          })\n",
    "      output_dict[key][\"total_cost\"]=round(total_fabric_cost,2)\n",
    "    \n",
    "    elif key ==\"description\":\n",
    "      output_dict[key]=\" \".join([x.strip() for x in item_breakup_dict[key].split(',')])\n",
    "    elif key ==\"buyer\":\n",
    "      output_dict[key]=item_breakup_dict[key].lower()\n",
    "      continue\n",
    "    elif key ==\"trims\":\n",
    "      \n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      total_trim_cost=0\n",
    "      if key not in output_dict.keys():\n",
    "        output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "        trim_quantity=float(predict_trim_quantity((buyer+\" \"+user_txt+ \" \"+item).strip()) )\n",
    "        trim_price_rate=predict_trim_rate((buyer+\" \"+user_txt+ \" \"+item).strip())\n",
    "         \n",
    "        total_cost+= trim_quantity*trim_price_rate\n",
    "        total_trim_cost+= trim_quantity*trim_price_rate\n",
    "        if round(trim_quantity,2)==0:\n",
    "            continue\n",
    "        else:\n",
    "            trim_quantity=round(trim_quantity,2)\n",
    "            \n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": round(trim_quantity,1),\n",
    "          \"rate\": round(trim_price_rate,3),\n",
    "          \"amount\": round(trim_quantity*trim_price_rate,2),\n",
    "          \"uom\": uom[item.strip()] if item.strip() in uom.keys() else \"\"\n",
    "        })\n",
    "        \n",
    "      output_dict[key][\"total_cost\"]=round(total_trim_cost,2)\n",
    "    elif key ==\"packing\":\n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      total_packing_cost=0\n",
    "      if key not in output_dict.keys():\n",
    "        output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "        price_rate=float(predict_trim_rate(user_txt+\" \"+item))\n",
    "        total_cost+= price_rate\n",
    "        total_packing_cost+=price_rate\n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": 1,\n",
    "          \"rate\": round(price_rate,3),\n",
    "          \"amount\": round(price_rate,2),\n",
    "          \"uom\": uom[item.strip()] if item.strip() in uom.keys() else \"\"\n",
    "        })\n",
    "      output_dict[key][\"total_cost\"]=round(total_packing_cost,2)\n",
    "    else:\n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      total_cmt_cost=0\n",
    "      if key not in output_dict.keys():\n",
    "        output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "        process_rate= float(predict_process_rate((buyer+\" \"+user_txt+ \" \"+item).strip()))\n",
    "        total_cost+=process_rate\n",
    "        total_cmt_cost+=process_rate\n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": 1,\n",
    "          \"rate\": round(process_rate,3),\n",
    "          \"amount\": round(process_rate,2),\n",
    "          \"uom\": uom[item.strip()] if item.strip() in uom.keys() else \"\"\n",
    "        })\n",
    "      output_dict[key][\"total_cost\"]=round(total_cmt_cost)\n",
    "  output_dict[\"Total cost\"] = round(total_cost,3)\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pqr'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buyer=\"abc\"\n",
    "buyer = buyer if len(buyer)>3 else \"pqr\"\n",
    "buyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_dict_with_cost(user_txt):\n",
    "  item_breakup_dict = get_item_breakup_dictionary_v4(user_txt)\n",
    "  \n",
    "  if not item_breakup_dict:\n",
    "    print(\"No valid breakup could be made from user text\")\n",
    "    return False\n",
    "\n",
    "  if \"buyer\" not in item_breakup_dict.keys():\n",
    "        item_breakup_dict[\"buyer\"]=\"\"\n",
    "        \n",
    "  user_txt = item_breakup_dict[\"description\"]\n",
    "  #print(user_txt)\n",
    "  total_cost=0\n",
    "  output_dict={}\n",
    "  for key in item_breakup_dict.keys():\n",
    "    if key==\"fabric\":\n",
    "      total_fabric_quantity=0\n",
    "      total_fabric_cost=0\n",
    "      used_fabrics=item_breakup_dict[key].split(',')\n",
    "      for fabric in used_fabrics:\n",
    "        fabric_quantity = float(predict_fabric_quantity((item_breakup_dict[\"buyer\"]+\" \"+user_txt+ \" \"+fabric).strip()))\n",
    "        print((item_breakup_dict[\"buyer\"]+\" \"+user_txt+ \" \"+fabric).strip(), \"HERE \", fabric_quantity)\n",
    "        total_fabric_quantity+=fabric_quantity\n",
    "        \n",
    "      if \"fabric\" not in output_dict.keys():\n",
    "        output_dict[\"fabric\"]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for fabric in used_fabrics:\n",
    "        fabric_quantity = float(predict_fabric_quantity((item_breakup_dict[\"buyer\"]+\" \"+user_txt+ \" \"+fabric).strip()))\n",
    "        if total_fabric_quantity>2.5 and (item_breakup_dict[\"buyer\"]+\" \"+user_txt+ \" \"+fabric).strip() not in improved_item_avgs.keys():\n",
    "            fabric_quantity= (fabric_quantity/total_fabric_quantity)*2.5\n",
    "        fabric_price_rate = predict_fabric_rate(user_txt+\" \"+fabric.strip() )\n",
    "        \n",
    "        total_cost+= fabric_quantity*fabric_price_rate\n",
    "        total_fabric_cost+=fabric_quantity*fabric_price_rate\n",
    "        output_dict[key][\"items\"].append({\"name\":fabric.strip(),\n",
    "                                          \"avg\": str(round(fabric_quantity,2))+\" meters\",\n",
    "                                          \"rate\": round(fabric_price_rate,3),\n",
    "                                          \"amount\": round(fabric_quantity*fabric_price_rate,2),\n",
    "                                          \"uom\": uom[fabric.strip()] if fabric.strip() in uom.keys() else \"\"\n",
    "                                          })\n",
    "      output_dict[key][\"total_cost\"]=round(total_fabric_cost,2)\n",
    "    \n",
    "    elif key ==\"description\":\n",
    "      output_dict[key]=\" \".join([x.strip() for x in item_breakup_dict[key].split(',')])\n",
    "    elif key ==\"buyer\":\n",
    "        output_dict[key]=item_breakup_dict[key].lower().strip()\n",
    "      \n",
    "    elif key ==\"trims\":\n",
    "      \n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      total_trim_cost=0\n",
    "    \n",
    "      if key not in output_dict.keys():\n",
    "        output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "        trim_quantity=float(predict_trim_quantity((item_breakup_dict[\"buyer\"]+\" \"+user_txt+\" \"+item).strip()) )\n",
    "        trim_price_rate=predict_trim_rate((item_breakup_dict[\"buyer\"]+\" \"+user_txt+\" \"+item).strip())\n",
    "       \n",
    "        total_cost+= trim_quantity*trim_price_rate\n",
    "        total_trim_cost+= trim_quantity*trim_price_rate\n",
    "        if round(trim_quantity,2)==0:\n",
    "            continue\n",
    "        else:\n",
    "            trim_quantity=round(trim_quantity,2)\n",
    "            \n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": round(trim_quantity,1),\n",
    "          \"rate\": round(trim_price_rate,3),\n",
    "          \"amount\": round(trim_quantity*trim_price_rate,2),\n",
    "          \"uom\": uom[item.strip()] if item.strip() in uom.keys() else \"\"\n",
    "        })\n",
    "        \n",
    "      output_dict[key][\"total_cost\"]=round(total_trim_cost,2)\n",
    "    elif key ==\"packing\":\n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      total_packing_cost=0\n",
    "      if key not in output_dict.keys():\n",
    "        output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "        price_rate=float(predict_trim_rate((item_breakup_dict[\"buyer\"]+\" \"+user_txt+\" \"+item).strip()))\n",
    "        total_cost+= price_rate\n",
    "        total_packing_cost+=price_rate\n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": 1,\n",
    "          \"rate\": round(price_rate,3),\n",
    "          \"amount\": round(price_rate,2),\n",
    "          \"uom\": uom[item.strip()] if item.strip() in uom.keys() else \"\"\n",
    "        })\n",
    "      output_dict[key][\"total_cost\"]=round(total_packing_cost,2)\n",
    "    else:\n",
    "      items = item_breakup_dict[key].split(',')\n",
    "      total_cmt_cost=0\n",
    "      if key not in output_dict.keys():\n",
    "        output_dict[key]={}\n",
    "      output_dict[key][\"items\"]=[]\n",
    "      for item in list(set(items)):\n",
    "        item= item.strip()\n",
    "        process_rate= float(predict_process_rate((item_breakup_dict[\"buyer\"]+\" \"+user_txt+\" \"+item).strip()))\n",
    "        total_cost+=process_rate\n",
    "        total_cmt_cost+=process_rate\n",
    "        output_dict[key][\"items\"].append({\n",
    "          \"name\":item.strip(),\n",
    "          \"avg\": 1,\n",
    "          \"rate\": round(process_rate,3),\n",
    "          \"amount\": round(process_rate,2),\n",
    "          \"uom\": uom[item.strip()] if item.strip() in uom.keys() else \"\"\n",
    "        })\n",
    "      output_dict[key][\"total_cost\"]=round(total_cmt_cost)\n",
    "  output_dict[\"Total cost\"] = round(total_cost,3)\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1599894"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fabric_quantity(\"saree silk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid breakup could be made from user text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict_with_cost(\"saree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_print_output(nested_dict):\n",
    "    output=\"======================================================================================================<br />\"\n",
    "    keys= [\"description\", \"fabric\", \"cmt\", \"trims\", \"packing\", \"total cost\"]\n",
    "    for key in keys:\n",
    "        if key not in nested_dict.keys():\n",
    "            continue\n",
    "        if type(nested_dict[key])==dict:\n",
    "          output+=( key.upper()+\" :\")+' <br />'\n",
    "          for k in nested_dict[key].keys():\n",
    "            if type(nested_dict[key][k])==dict:\n",
    "              output+=(\"&nbsp&nbsp&nbsp&nbsp\"+k+\" :\")+ ' <br />'\n",
    "              for i in nested_dict[key][k].keys():\n",
    "                if type(nested_dict[key][k][i])==dict:\n",
    "                  output+=(\"&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp\"+i + \" : \")+' <br />'\n",
    "                  for j in nested_dict[key][k][i].keys():\n",
    "                    output+=(\"&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp\"+j+\": \"+str(nested_dict[key][k][i][j]))+'<br />'\n",
    "                else:\n",
    "                  output+=(\"&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp\"+i+\": \"+str(nested_dict[key][k][i]))+ '<br />'\n",
    "            else:\n",
    "              output+= ' <br />--------------------------------------------------------------------------------------- <br />'\n",
    "              output+=(\"&nbsp&nbsp&nbsp&nbsp\"+str(k)+\" : \"+str(nested_dict[key][k]))+ ' <br />'\n",
    "              output+= ' <br />--------------------------------------------------------------------------------------- <br />'\n",
    "        else:\n",
    "          output+=(key.upper()+\" : \"+str(nested_dict[key]))+' <br />'\n",
    "        output+= ' <br />'\n",
    "        output+=\"=====================================================================================================<br />\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:41:56.655454  5444 _internal.py:122]  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://6ab9d490.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:42:05.273142  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "I0924 12:42:06.119115  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:06] \"\u001b[33mGET /static/2_blur.jpg HTTP/1.1\u001b[0m\" 404 -\n",
      "I0924 12:42:11.027162  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:11] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'email': 'satyam', 'password': 'abc'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:42:11.868487  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:11] \"\u001b[37mGET /home HTTP/1.1\u001b[0m\" 200 -\n",
      "I0924 12:42:15.761837  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:15] \"\u001b[32mPOST /user_input HTTP/1.1\u001b[0m\" 302 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION <SecureCookieSession {'_flashes': [('message', 'Please log in to access this page.')], '_fresh': True, '_id': '5e7e9b19d5809fc5ec99a420ea29db1fdd874d5f60117883cfe536d119412ce0a6f74eb8f9b19281cc2fc9740ba4e8a81add738b8a078bee8c7c6663d236a13a', 'cache': {}, 'nested_dict': {}, 'user_garment_description': 'shirt', 'user_garment_text_description': 'shirt', 'user_id': 'satyam', 'username': 'satyam'}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:42:16.665443  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:16] \"\u001b[37mGET /success/shirt HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM IMPROVED DATABASE used the breakup text corresponding to the key  shirt -with breakup text:  shirt\n",
      "{'description': 'shirt'}\n",
      "SESSONS DICT:  {'description': 'shirt', 'Total cost': 0, 'buyer': 'apple', 'fabric': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}, 'trims': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}, 'cmt': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}, 'packing': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:42:21.732465  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:21] \"\u001b[37mGET /success/improve/shirt HTTP/1.1\u001b[0m\" 200 -\n",
      "[2019-09-24 12:42:49,915] ERROR in app: Exception on /improved [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1982, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1615, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1632, in finalize_request\n",
      "    response = self.process_response(response)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1858, in process_response\n",
      "    self.save_session(ctx.session, response)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 924, in save_session\n",
      "    return self.session_interface.save_session(self, session, response)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\sessions.py\", line 363, in save_session\n",
      "    val = self.get_signing_serializer(app).dumps(dict(session))\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\itsdangerous\\serializer.py\", line 166, in dumps\n",
      "    payload = want_bytes(self.dump_payload(obj))\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\itsdangerous\\url_safe.py\", line 42, in dump_payload\n",
      "    json = super(URLSafeSerializerMixin, self).dump_payload(obj)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\itsdangerous\\serializer.py\", line 133, in dump_payload\n",
      "    return want_bytes(self.serializer.dumps(obj, **self.serializer_kwargs))\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\sessions.py\", line 85, in dumps\n",
      "    return json.dumps(_tag(value), separators=(',', ':'))\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\json.py\", line 123, in dumps\n",
      "    rv = _json.dumps(obj, **kwargs)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\json\\__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\json\\encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\json\\encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\site-packages\\flask\\json.py\", line 80, in default\n",
      "    return _json.JSONEncoder.default(self, o)\n",
      "  File \"C:\\Users\\geuser\\Anaconda3\\lib\\json\\encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type ObjectId is not JSON serializable\n",
      "I0924 12:42:49.987201  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:49] \"\u001b[1m\u001b[35mPOST /improved HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION:  <SecureCookieSession {'_flashes': [('message', 'Please log in to access this page.')], '_fresh': True, '_id': '5e7e9b19d5809fc5ec99a420ea29db1fdd874d5f60117883cfe536d119412ce0a6f74eb8f9b19281cc2fc9740ba4e8a81add738b8a078bee8c7c6663d236a13a', 'cache': {'shirt': {'Total cost': 0, 'buyer': 'apple', 'cmt': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}, 'description': 'shirt', 'fabric': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}, 'packing': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}, 'trims': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}}}, 'nested_dict': {'Total cost': 0, 'buyer': 'apple', 'cmt': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}, 'description': 'shirt', 'fabric': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}, 'packing': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}, 'trims': {'items': [{'amount': 0, 'avg': 0, 'name': '-', 'rate': 0, 'uom': ''}], 'total_cost': 0}}, 'user_garment_description': 'shirt', 'user_garment_text_description': 'shirt', 'user_id': 'satyam', 'username': 'satyam', 'improved_dict': {'cmt': {'total_cost': '0', 'items': [{'name': '', 'avg': '0', 'rate': '0', 'amount': '0', 'UOM': ''}]}, 'fabric': {'total_cost': '72', 'items': [{'name': 'cotton voil', 'avg': '1.2', 'rate': '60', 'amount': '72', 'UOM': ''}]}, 'packing': {'total_cost': '0', 'items': []}, 'trims': {'total_cost': '0', 'items': []}, 'description': 'shirt', 'buyer': 'APPLE', 'Total cost': 72.0, 'user': 'satyam', 'user_garment_text_description': 'shirt', 'username': 'satyam', '_id': ObjectId('5d89c1f11615a73978b87005')}}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:42:50.788091  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "I0924 12:42:51.630830  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:51] \"\u001b[33mGET /static/2_blur.jpg HTTP/1.1\u001b[0m\" 404 -\n",
      "I0924 12:42:59.944244  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:42:59] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'email': 'satyam', 'password': 'abc'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:43:00.734265  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:43:00] \"\u001b[37mGET /home HTTP/1.1\u001b[0m\" 200 -\n",
      "I0924 12:43:12.337655  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:43:12] \"\u001b[37mGET /logout HTTP/1.1\u001b[0m\" 200 -\n",
      "I0924 12:43:13.168673  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:43:13] \"\u001b[33mGET /static/2_blur.jpg HTTP/1.1\u001b[0m\" 404 -\n",
      "I0924 12:43:49.156169  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:43:49] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'email': 'pranjal', 'password': 'xyz'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:43:49.946825  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:43:49] \"\u001b[37mGET /home HTTP/1.1\u001b[0m\" 200 -\n",
      "I0924 12:43:57.336622  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:43:57] \"\u001b[32mPOST /user_input HTTP/1.1\u001b[0m\" 302 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION <SecureCookieSession {'_fresh': True, '_id': '5e7e9b19d5809fc5ec99a420ea29db1fdd874d5f60117883cfe536d119412ce0a6f74eb8f9b19281cc2fc9740ba4e8a81add738b8a078bee8c7c6663d236a13a', 'cache': {}, 'nested_dict': {}, 'user_id': 'pranjal', 'username': 'pranjal'}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0924 12:43:58.139011  5444 _internal.py:122] 127.0.0.1 - - [24/Sep/2019 12:43:58] \"\u001b[37mGET /success/shirt HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM IMPROVED DATABASE used the breakup text corresponding to the key  shirt -with breakup text:  shirt\n",
      "{'description': 'shirt'}\n",
      "SESSONS DICT:  {'description': 'shirt', 'Total cost': 0, 'buyer': 'mango', 'fabric': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}, 'trims': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}, 'cmt': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}, 'packing': {'items': [{'name': '-', 'avg': 0, 'rate': 0, 'amount': 0, 'uom': ''}], 'total_cost': 0}}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from flask import*\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask_login import LoginManager, UserMixin, login_required, login_user, logout_user \n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from flask_login import LoginManager, UserMixin, login_required, login_user, logout_user \n",
    "app = Flask(__name__)\n",
    "app.secret_key = \"bluekaktus_secret\"\n",
    "\n",
    "# flask-login\n",
    "login_manager = LoginManager()\n",
    "login_manager.init_app(app)\n",
    "login_manager.login_view = \"login\"\n",
    "\n",
    "global users\n",
    "\n",
    "run_with_ngrok(app)\n",
    "# silly user model\n",
    "class User(UserMixin):\n",
    "    pass\n",
    "\n",
    "@login_manager.user_loader\n",
    "def user_loader(email):\n",
    "    if email not in users:\n",
    "        return\n",
    "\n",
    "    user = User()\n",
    "    user.id = email\n",
    "    return user\n",
    "\n",
    "@app.route('/success/<string_query>',methods = ['POST', 'GET']) \n",
    "def success(string_query): \n",
    "    if request.method == 'POST': \n",
    "        return redirect(url_for('improve'), user_text_description=string_query) #user_text_description is argument of 'improve'\n",
    "    \n",
    "    user_specific_item_breakup_database={}\n",
    "    if session[\"username\"] in improved_item_breakup_database.keys():\n",
    "        user_specific_item_breakup_database=improved_item_breakup_database[session[\"username\"]]\n",
    "    else:\n",
    "        print(\"First entry by \", session[\"username\"].upper())\n",
    "        \n",
    "        \n",
    "    nested_dict={}\n",
    "    cache=session[\"cache\"]\n",
    "########################## If the exact key is already avalable in the database where user is entering improved records########\n",
    "    if string_query in user_specific_item_breakup_database.keys():\n",
    "        item_breakup_text=user_specific_item_breakup_database[string_query][\"underscore_seperated\"]\n",
    "        buyer=user_specific_item_breakup_database[string_query][\"buyer\"]\n",
    "\n",
    "        print(\"FROM IMPROVED DATABASE used the breakup text corresponding to the key \",string_query, \"-with breakup text: \",item_breakup_text)\n",
    "        item_breakup_dict,_=get_label_dict(item_breakup_text)\n",
    "        nested_dict=output_dict_with_cost_from_item_breakup_dic(item_breakup_dict, buyer=buyer)\n",
    "        nested_dict[\"buyer\"]=buyer\n",
    "        \n",
    "########################## If somewhat similar key is already avalable in the database where user is entering improved records########        \n",
    "    elif most_similar_key_for_text_from_dict(string_query,user_specific_item_breakup_database):\n",
    "        most_similar_key=most_similar_key_for_text_from_dict(string_query,user_specific_item_breakup_database)\n",
    "        item_breakup_text=user_specific_item_breakup_database[most_similar_key][\"underscore_seperated\"]\n",
    "        buyer=user_specific_item_breakup_database[most_similar_key][\"buyer\"]\n",
    "        print(\"FROM IMPROVED DATABASE used the breakup text corresponding to the key \",most_similar_key, \"-with breakup text: \",item_breakup_text)\n",
    "        \n",
    "        ############# Adding additional user inputted items into the string queried from imroved_item_breakup_database#######\n",
    "        initial_len=len(item_breakup_text.split())\n",
    "        user_query_dict,_=get_label_dict(string_query)\n",
    "        for key in user_query_dict.keys():\n",
    "            if key==\"description\":\n",
    "                continue\n",
    "            values= [\"_\".join(x.strip().split()) for x in user_query_dict[key].split(',')]\n",
    "            item_breakup_text=\" \".join(list(set(item_breakup_text.split()+values)))\n",
    "        if len(item_breakup_text.split())!=initial_len:\n",
    "            print(\"ADDED ADDITIONAL USER INPUT INFO FROM USER INPUT: \",user_query_dict)\n",
    "\n",
    "   \n",
    "        item_breakup_dict,_=get_label_dict(item_breakup_text)\n",
    "        nested_dict=output_dict_with_cost_from_item_breakup_dic(item_breakup_dict, buyer=buyer) \n",
    "        nested_dict[\"buyer\"]=buyer\n",
    "#############################################################################################################################       \n",
    "    elif string_query in cache.keys() and cache[string_query][\"user\"]==session[\"username\"]:\n",
    "        nested_dict=cache[string_query]\n",
    "        \n",
    "    else:\n",
    "#         nested_dict= output_dict_with_cost(str(string_query))\n",
    "#         print(\"used function: output_dict_with_cost() \",nested_dict)\n",
    "        nested_dict=False\n",
    "        \n",
    "################################ If user inputs an entirely new garment #####################################################\n",
    "    if not nested_dict:\n",
    "        nested_dict=dummy_output_dict(string_query)\n",
    "        session[\"cache\"]=cache\n",
    "        session[\"nested_dict\"]=nested_dict\n",
    "        print(\"SESSONS DICT: \", session[\"nested_dict\"])\n",
    "        return render_template('result.html', result=nested_dict, garment_text_description=string_query+\"(new entry)\")\n",
    "##############################################################################################################################\n",
    "    dummy_dict=dummy_output_dict(string_query)\n",
    "    \n",
    "    if \"fabric\" not in nested_dict.keys(): \n",
    "        nested_dict[\"fabric\"]=dummy_dict[\"fabric\"]\n",
    "    if \"trims\" not in nested_dict.keys():\n",
    "        nested_dict[\"trims\"]=dummy_dict[\"trims\"]\n",
    "    if \"cmt\" not in nested_dict.keys():\n",
    "        nested_dict[\"cmt\"]=dummy_dict[\"cmt\"]\n",
    "    if \"packing\" not in nested_dict.keys():\n",
    "        nested_dict[\"packing\"]=dummy_dict[\"packing\"] \n",
    "    if \"buyer\" not in nested_dict.keys():\n",
    "        nested_dict[\"buyer\"]=\"general\"\n",
    "    \n",
    "    cache[string_query]=nested_dict \n",
    "    session[\"cache\"]=cache\n",
    "    session[\"nested_dict\"]=nested_dict\n",
    "    print(\"SESSONS DICT: \", session[\"nested_dict\"])\n",
    "    return render_template('result.html', result=nested_dict, garment_text_description=string_query)\n",
    "    #garment_text_description is the variable of result.html \n",
    "\n",
    "@app.route('/success/improve/<user_text_description>',methods = ['POST', 'GET']) \n",
    "def improve(user_text_description): \n",
    "    \n",
    "#     if request.method == 'POST':                                  #Handled by javascript fetch() function\n",
    "#         return redirect(url_for('improved'))\n",
    "    nested_dict=session[\"nested_dict\"]\n",
    " \n",
    "    return render_template('improve_with_bootstrap.html', result=nested_dict, garment_text_description=user_text_description)\n",
    "    #garment_text_description is a variable of the improve.html file\n",
    "\n",
    "@app.route('/improved',methods = ['POST','GET']) \n",
    "def improved(): \n",
    "    \n",
    "    global improved_item_rates\n",
    "    global improved_item_avgs\n",
    "    global new_label_classification_data\n",
    "    global improved_item_breakup_database\n",
    "    global space_clean_corpus\n",
    "    global underscored_corpus\n",
    "    global uom\n",
    "    \n",
    "    improved_dict=request.get_json()['dtData']\n",
    "    \n",
    "    item_breakup_texts=item_beakup_text_from_dict(improved_dict)\n",
    "    login_info=session[\"username\"]\n",
    "    improved_dict[\"user\"]=login_info.lower()\n",
    "    session[\"improved_dict\"]=improved_dict\n",
    "   \n",
    "    \n",
    "    ############################### If user enetered dashes without_values###################################################\n",
    "    for key in improved_dict.keys():\n",
    "        if key in [\"description\", \"Total cost\", \"buyer\", \"user\"]:\n",
    "            continue\n",
    "        for item in improved_dict[key][\"items\"]:\n",
    "            if '-' in [item[\"name\"],item[\"avg\"], item[\"avg\"]]:\n",
    "                improved_dict[key][\"items\"].remove(item)\n",
    "                \n",
    "  ##########################################################################################################################\n",
    "   \n",
    "    \n",
    "  ######A new garment information  entered by user stored. Buyer is stored seperately in the text ######################### \n",
    "    new_breakup_text_dict=item_beakup_text_from_dict(improved_dict)\n",
    "    buyer=new_breakup_text_dict[\"buyer\"]\n",
    "    \n",
    "    user_garment_text_description=session[\"user_garment_text_description\"]\n",
    "    #improved_item_breakup_database[user_garment_text_description]=new_breakup_text_dict\n",
    "    if session[\"username\"] not in improved_item_breakup_database.keys():\n",
    "        improved_item_breakup_database[session[\"username\"]]={}\n",
    "    improved_item_breakup_database[session[\"username\"]][user_garment_text_description]=new_breakup_text_dict\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    underscored_corpus=list(underscored_corpus)\n",
    "    underscored_corpus.append((new_breakup_text_dict[\"underscore_seperated\"]+\" \"+new_breakup_text_dict[\"buyer\"]).strip())\n",
    "    space_clean_corpus=list(space_clean_corpus)\n",
    "    space_clean_corpus.append((new_breakup_text_dict[\"space_seperated\"]+\" \"+new_breakup_text_dict[\"buyer\"]).strip())                                   \n",
    "    underscored_corpus=np.array(underscored_corpus)\n",
    "    space_clean_corpus=np.array(space_clean_corpus)\n",
    "    \n",
    "    \n",
    "    improved_item_rates=merge_dictionaries(current_dictionary=item_rates_from_dict(improved_dict), \n",
    "                                           database_dictionary=improved_item_rates)\n",
    "    improved_item_avgs=merge_dictionaries(current_dictionary=item_avgs_from_dict(improved_dict), \n",
    "                                           database_dictionary=improved_item_avgs)\n",
    "    #try:\n",
    "    uom= merge_dictionaries(current_dictionary=uom_from_dict(improved_dict), \n",
    "                                               database_dictionary=uom)\n",
    "    #except:\n",
    "        #print(\"UOM not added\")\n",
    "    \n",
    "    new_item_label_dict=item_labels_from_dict(improved_dict)\n",
    "    ################################### Variables/arrays of texts used previously by these names#############################\n",
    "\n",
    "\n",
    "          \n",
    "    ######## Finding new user entered items/products/terms which were not in database previously and \n",
    "    ###########################################################adding them to the new_label_classification_dictionary##########\n",
    "    for i in range(len(new_item_label_dict[\"product_info\"])):\n",
    "        #if new_item_label_dict[\"product_info\"][i] not in label_classifier_vectorizer.vocabulary_.keys():\n",
    "        if new_item_label_dict[\"product_info\"][i] not in new_label_classification_data[\"product_info\"]:\n",
    "            new_label_classification_data[\"product_info\"].append(new_item_label_dict[\"product_info\"][i])\n",
    "            new_label_classification_data[\"label\"].append(new_item_label_dict[\"label\"][i])\n",
    "    ###########################################################################################################################\n",
    "    \n",
    "    ##########################putting the improved in mongo database collection named  user_improved_costing_collection #######\n",
    "    improved_dict[\"user_garment_text_description\"]=user_garment_text_description\n",
    "    improved_dict[\"Total cost\"]=float((improved_dict[\"Total cost\"].strip()).split()[-1])\n",
    "    improved_dict[\"username\"]=session[\"username\"]\n",
    "    user_improved_costing_collection.insert_one(improved_dict)\n",
    "    ##########################################################################################################################\n",
    "    print(\"SESSION: \", session)\n",
    "    return \"Error if not string\" \n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def login():\n",
    "    login_dict={}\n",
    "    if request.method==\"POST\":\n",
    "        global users\n",
    "        users=list(client[\"costing_engine_db_with_login\"][\"users\"].find({\"database_name\":\"username_password\"}))[0]\n",
    "        login_dict=request.form.to_dict()\n",
    "\n",
    "        if login_dict[\"email\"] in users.keys() and login_dict['password'] == users[login_dict[\"email\"]]['password']:\n",
    "            print(login_dict)\n",
    "            email = login_dict['email']\n",
    "            user = User()\n",
    "            user.id = email\n",
    "            login_user(user)\n",
    "            session[\"username\"]=email\n",
    "            session[\"cache\"]={}\n",
    "            session[\"nested_dict\"]={}\n",
    "            return redirect(url_for('homepage'))\n",
    "        \n",
    "        return render_template(\"login.html\", error_message=\"Incorrect credentials\")\n",
    "\n",
    " \n",
    "    return render_template(\"login.html\")\n",
    "\n",
    "    \n",
    "  \n",
    "@app.route('/logout')\n",
    "def logout():\n",
    "    session.clear()\n",
    "    logout_user()\n",
    "    return render_template(\"login.html\", error_message=\"Logged out!\")\n",
    "\n",
    "@app.route('/home', methods=['POST', 'GET'])\n",
    "@login_required\n",
    "def homepage():\n",
    "    return render_template(\"index.html\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@app.route('/user_input',methods = ['POST', 'GET']) \n",
    "def string_query_entry():  \n",
    "    print(\"SESSION \"+ str(session))\n",
    "    if request.method == 'POST': \n",
    "        user = request.form['string_query'] \n",
    "        session[\"user_garment_text_description\"]=user\n",
    "        \n",
    "        return redirect(url_for('success',string_query = user)) #string_query is a variable of the html file rendered in function 'success'\n",
    "    else: \n",
    "        user = request.args.get('string_query') \n",
    "        return redirect(url_for('success',string_query = user)) \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    app.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_item_breakup_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_output.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
